# Health-Evals: Scalable Evaluation Harness for Wellness Coaching LLMs

**A reproducible evaluation system** for comparing wellness-coaching LLMs end-to-end:
- A/B **model comparisons** (MUT vs Baseline) with **schema-validated** outputs
- **LLM-as-a-Judge** scoring (rubric + self-consistency)
- **Automatic** safety/quality metrics
- **Reference-based** text metrics (ROUGE-L, BERTScore, embedding cosine, PPL)
- **Human evaluation UI** (blind A/B + Likert + error tags)
- **Ray** for parallel, scalable batch inference
- **Provider-agnostic** (Ollama local, OpenAI hosted, vLLM compatible)

> **Current default config:**  
> **MUT (Model Under Test)** â€” `llama3:8b` via **Ollama** (local, macOS-friendly)  
> **Baseline & Judge** â€” **OpenAI** (`gpt-4o-mini` + `gpt-4o`)  
> You can change all of this via **`configs/model.yaml`** only.

---

## ðŸ”Ž What this repo does

You give the system a set of daily signals (e.g., RHR/HRV/sleep/steps + a journal snippet).  
It **prompts** two models (MUT & Baseline) with *the same coaching task*, **forces JSON** outputs that match a Pydantic schema (summary â†’ suggestions â†’ steps â†’ disclaimer), then **scores** the results with:
- an **LLM judge** (multi-prompt self-consistency + weighted rubric),
- **cheap automatic checks** (blocked-rate, disclaimer compliance, length, safety regex), and
- **reference-based metrics** against a silver reference set generated by the judge.

All artifacts (raw/parsed outputs, scores, metrics) are written to `out/` so you can track quality diffs over time.

---

## ðŸ§­ Quickstart

> macOS, Python 3.11+, Ollama installed (`brew install ollama`), OpenAI key for baseline/judge

```bash
# 0) Env
python3 -m venv .venv && source .venv/bin/activate
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt
pip install -U "pyarrow==16.1.0" "ray[data]==2.20.0"

# 1) Local MUT
ollama serve            # keep open in its own terminal
ollama pull llama3:8b

# 2) OpenAI (baseline/judge)
echo 'OPENAI_API_KEY=sk-...YOUR_KEY...' >> .env

# 3) Data
python scripts/make_synthetic_data.py
python scripts/prepare_eval_splits.py

# 4) Batch inference (A/B) + Guardrails schema validation
python evals/runners/batch_infer.py

# 5) Judge scoring (rubric + self-consistency)
python evals/runners/eval_llm_judge.py

# 6) Automatic deterministic checks
python evals/runners/eval_auto.py

# 7) Silver references & text metrics
python scripts/refs_from_judge.py --limit 50
python evals/runners/eval_ref_metrics.py --skip-ppl --limit 50

# 8) Human evaluation UI
streamlit run human/annotator_app.py          # saves to out/human/annotations.csv
python scripts/analyze_human_eval.py

# 9) Scale out with Ray

python evals/runners/ray_eval.py \
  --config configs/model.yaml \
  --split evals/datasets/test.jsonl \
  --prompt coach_v1.jinja \
  --tag mut_v1 \
  --outdir out/ray/mut_v1 \
  --num-actors 3 --batch-size 4 --limit 20

python scripts/collect_ray_outputs.py out/ray/mut_v1 out/infer/mut_v1.ray.jsonl
mv out/infer/mut_v1.ray.jsonl out/infer/mut_v1.jsonl
python evals/runners/eval_llm_judge.py
python evals/runners/eval_auto.py
```

## Pipeline Overview

![alt text](https://github.com/mohandasnj/health-evals/blob/main/img/pipeline.png)

## Providers Abstraction

## End-to-End Run

## Key Files

configs/model.yaml â€” single source of truth for models & providers (MUT, baseline, judge)

configs/judge.yaml â€” rubric dimensions, weights, scale

apps/providers.py â€” provider abstraction (Ollama/OpenAI/vLLM) with retries

apps/wellness_coach/schemas.py â€” Pydantic (v2) schema used by Guardrails

apps/wellness_coach/prompt_templates/coach_v1.jinja â€” JSON-only prompt (MUT)

apps/wellness_coach/prompt_templates/coach_v2.jinja â€” coachable/motivational variant (Baseline)

evals/runners/batch_infer.py â€” renders â†’ calls providers â†’ validates â†’ writes out/infer/

evals/runners/eval_llm_judge.py â€” rubric scoring with self-consistency â†’ out/judged/

evals/runners/eval_auto.py â€” deterministic safety/quality checks â†’ out/metrics/

evals/runners/ray_eval.py â€” scalable parallel inference (MUT)

scripts/refs_from_judge.py â€” builds silver references with the judge

evals/runners/eval_ref_metrics.py â€” ROUGE-L, BERTScore, embedding cosine, PPL â†’ out/metrics_ref/

human/annotator_app.py â€” Streamlit blind A/B + Likert + error tags â†’ out/human/

scripts/analyze_human_eval.py â€” aggregates human ratings

## Results

Look at the files in the out folder corresponding to each statistic. I only put a small test of 5 entries, but there are about 300 entries of test data total. From this small sample, the results show that GPT-4o-mini performs a bit better than Ollama. There could be bias here as well because the judge is another OpenAI model. Further research can be done with different models for both the MUT, baseline, and judge. 

## Streamlit Demo

To run: streamlit run human/annotator_app.py

![alt text](https://github.com/mohandasnj/health-evals/blob/main/img/streamlit1.png)
![alt text](https://github.com/mohandasnj/health-evals/blob/main/img/streamlit2.png)

## ðŸ§  Methods implemented (and evidence)

LLM-as-Judge with self-consistency

  Code: evals/runners/eval_llm_judge.py, configs/judge.yaml
  
  Output: out/judged/*.jl
  
Automatic metric-based evals
  
  Heuristics: evals/runners/eval_auto.py â†’ out/metrics/*.csv

  Text metrics: evals/runners/eval_ref_metrics.py â†’ out/metrics_ref/*.csv
  (ROUGE-L, BERTScore-F1, embedding cosine, optional PPL)

Human evaluation (blind A/B, Likert, error tags)
  
  UI: human/annotator_app.py (Streamlit), data: out/human/annotations.csv

  Aggregation: scripts/analyze_human_eval.py

Scalability

  Parallel inference: evals/runners/ray_eval.py, shards merged via scripts/collect_ray_outputs.py

Guardrails + schema enforcement

  Pydantic v2 schema: apps/wellness_coach/schemas.py

  JSON-only prompts: apps/wellness_coach/prompt_templates/*.jinja

## ðŸ§­ Next Steps (roadmap)


