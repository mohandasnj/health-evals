# Health-Evals: Scalable Evaluation Harness for Wellness Coaching LLMs

**A reproducible evaluation system** for comparing wellness-coaching LLMs end-to-end:
- A/B **model comparisons** (MUT vs Baseline) with **schema-validated** outputs
- **LLM-as-a-Judge** scoring (rubric + self-consistency)
- **Automatic** safety/quality metrics
- **Reference-based** text metrics (ROUGE-L, BERTScore, embedding cosine, PPL)
- **Human evaluation UI** (blind A/B + Likert + error tags)
- **Ray** for parallel, scalable batch inference
- **Provider-agnostic** (Ollama local, OpenAI hosted, vLLM compatible)

> **Current default config:**  
> **MUT (Model Under Test)** â€” `llama3:8b` via **Ollama** (local, macOS-friendly)  
> **Baseline & Judge** â€” **OpenAI** (`gpt-4o-mini` + `gpt-4o`)  
> You can change all of this via **`configs/model.yaml`** only.

---

## ðŸ”Ž What this repo does

You give the system a set of daily signals (e.g., RHR/HRV/sleep/steps + a journal snippet).  
It **prompts** two models (MUT & Baseline) with *the same coaching task*, **forces JSON** outputs that match a Pydantic schema (summary â†’ suggestions â†’ steps â†’ disclaimer), then **scores** the results with:
- an **LLM judge** (multi-prompt self-consistency + weighted rubric),
- **cheap automatic checks** (blocked-rate, disclaimer compliance, length, safety regex), and
- **reference-based metrics** against a silver reference set generated by the judge.

All artifacts (raw/parsed outputs, scores, metrics) are written to `out/` so you can track quality diffs over time.

---

## ðŸ§­ Quickstart

> macOS, Python 3.11+, Ollama installed (`brew install ollama`), OpenAI key for baseline/judge

```bash
# 0) Env
python3 -m venv .venv && source .venv/bin/activate
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt
pip install -U "pyarrow==16.1.0" "ray[data]==2.20.0"

# 1) Local MUT
ollama serve            # keep open in its own terminal
ollama pull llama3:8b

# 2) OpenAI (baseline/judge)
echo 'OPENAI_API_KEY=sk-...YOUR_KEY...' >> .env

# 3) Data
python scripts/make_synthetic_data.py
python scripts/prepare_eval_splits.py

# 4) Batch inference (A/B) + Guardrails schema validation
python evals/runners/batch_infer.py

# 5) Judge scoring (rubric + self-consistency)
python evals/runners/eval_llm_judge.py

# 6) Automatic deterministic checks
python evals/runners/eval_auto.py

# 7) Silver references & text metrics
python scripts/refs_from_judge.py --limit 50
python evals/runners/eval_ref_metrics.py --skip-ppl --limit 50

# 8) Human evaluation UI
streamlit run human/annotator_app.py          # saves to out/human/annotations.csv
python scripts/analyze_human_eval.py

# 9) Scale out with Ray

python evals/runners/ray_eval.py \
  --config configs/model.yaml \
  --split evals/datasets/test.jsonl \
  --prompt coach_v1.jinja \
  --tag mut_v1 \
  --outdir out/ray/mut_v1 \
  --num-actors 3 --batch-size 4 --limit 20

python scripts/collect_ray_outputs.py out/ray/mut_v1 out/infer/mut_v1.ray.jsonl
mv out/infer/mut_v1.ray.jsonl out/infer/mut_v1.jsonl
python evals/runners/eval_llm_judge.py
python evals/runners/eval_auto.py

## Pipeline Overview
<img width="1732" height="516" alt="image" src="https://github.com/user-attachments/assets/71a7953c-9d6a-4651-93a6-94a5f9ea174d" />
