# Health-Evals: Scalable Evaluation Harness for Wellness Coaching LLMs

**A reproducible evaluation system** for comparing wellness-coaching LLMs end-to-end:
- A/B **model comparisons** (LLM vs Baseline) with **schema-validated** outputs
- **LLM-as-a-Judge** scoring (rubric + self-consistency)
- **Automatic** safety/quality metrics
- **Reference-based** text metrics (ROUGE-L, BERTScore, embedding cosine, PPL)
- **Human evaluation UI** (blind A/B + Likert + error tags)
- **Ray** for parallel, scalable batch inference
- **Provider-agnostic** (Ollama local, OpenAI hosted, vLLM compatible)

> **Current default config:**  
> **LLM (Model Under Test)** â€” `llama3:8b` via **Ollama** (local, macOS-friendly)  
> **Baseline & Judge** â€” **OpenAI** (`gpt-4o-mini` + `gpt-4o`)  
> You can change all of this via **`configs/model.yaml`** only.

---

## ðŸ”Ž What this repo does

You give the system a set of daily signals (e.g., RHR/HRV/sleep/steps + a journal snippet).  
It **prompts** two models (LLM & Baseline) with *the same coaching task*, **forces JSON** outputs that match a Pydantic schema (summary â†’ suggestions â†’ steps â†’ disclaimer), then **scores** the results with:
- an **LLM judge** (multi-prompt self-consistency + weighted rubric),
- **cheap automatic checks** (blocked-rate, disclaimer compliance, length, safety regex), and
- **reference-based metrics** against a silver reference set generated by the judge.

All artifacts (raw/parsed outputs, scores, metrics) are written to `out/` so you can track quality diffs over time.

---

## ðŸ§­ Quickstart

> macOS, Python 3.11+, Ollama installed (`brew install ollama`), OpenAI key for baseline/judge

```bash
# 0) Env
python3 -m venv .venv && source .venv/bin/activate
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt
pip install -U "pyarrow==16.1.0" "ray[data]==2.20.0"

# 1) Local LLM
ollama serve            # keep open in its own terminal
ollama pull llama3:8b

# 2) OpenAI (baseline/judge)
echo 'OPENAI_API_KEY=sk-...YOUR_KEY...' >> .env

# 3) Data
python scripts/make_synthetic_data.py
python scripts/prepare_eval_splits.py

# 4) Batch inference (A/B) + Guardrails schema validation
python evals/runners/batch_infer.py

# 5) Judge scoring (rubric + self-consistency)
python evals/runners/eval_llm_judge.py

# 6) Automatic deterministic checks
python evals/runners/eval_auto.py

# 7) Silver references & text metrics
python scripts/refs_from_judge.py --limit 50
python evals/runners/eval_ref_metrics.py --skip-ppl --limit 50

# 8) Human evaluation UI
streamlit run human/annotator_app.py          # saves to out/human/annotations.csv
python scripts/analyze_human_eval.py

# 9) Scale out with Ray

python evals/runners/ray_eval.py \
  --config configs/model.yaml \
  --split evals/datasets/test.jsonl \
  --prompt coach_v1.jinja \
  --tag mut_v1 \
  --outdir out/ray/mut_v1 \
  --num-actors 3 --batch-size 4 --limit 20

python scripts/collect_ray_outputs.py out/ray/mut_v1 out/infer/mut_v1.ray.jsonl
mv out/infer/mut_v1.ray.jsonl out/infer/mut_v1.jsonl
python evals/runners/eval_llm_judge.py
python evals/runners/eval_auto.py
```

## Pipeline Overview

![alt text](https://github.com/mohandasnj/health-evals/blob/main/img/pipeline.png)

## Providers Abstraction

![alt text](https://github.com/mohandasnj/health-evals/blob/main/img/providers.png)

## End-to-End Run

![alt text](https://github.com/mohandasnj/health-evals/blob/main/img/sequence.png)

## Key Files

Configs

  * model.yaml â€” models & providers (local LLM, baseline, judge)
  
  * judge.yaml â€” rubric dimensions, weights, scaling

Apps (core logic)

  * providers.py â€” provider abstraction (Ollama/OpenAI/vLLM) with retries
  
  * wellness_coach/schemas.py â€” Pydantic v2 schema (Guardrails contracts)
  
  * wellness_coach/prompt_templates/coach_v1.jinja â€” JSON-only prompt (local)
  
  * wellness_coach/prompt_templates/coach_v2.jinja â€” coachable variant (baseline)

Evals / Runners

  * batch_infer.py â€” render prompts â†’ call providers â†’ validate â†’ out/infer/*.jsonl
  
  * eval_llm_judge.py â€” rubric scoring (N=3 self-consistency) â†’ out/judged/*.jl
  
  * eval_auto.py â€” deterministic checks â†’ out/metrics/*.csv
  
  * eval_ref_metrics.py â€” ROUGE-L, BERTScore, embedding cosine, (optional) PPL â†’ out/metrics_ref/*.csv
  
  * ray_eval.py â€” parallel local inference with Ray

Scripts & Data

  * make_synthetic_data.py, scripts/prepare_eval_splits.py â€” toy data & splits
  
  * refs_from_judge.py â€” build silver references via the judge
  
  * xollect_ray_outputs.py â€” merge Ray shards
  
  * analyze_human_eval.py â€” aggregate human ratings
  
  * evals/datasets/*.jsonl â€” input datasets

Human Evaluation

  * annotator_app.py â€” Streamlit blind A/B rating UI â†’ out/human/annotations.csv

## Results

Look at the files in the out folder corresponding to each statistic. I only put a small test of 5 entries, but there are about 300 entries of test data total. From this small sample, the results show that GPT-4o-mini performs a bit better than Ollama. There could be bias here as well because the judge is another OpenAI model. Further research can be done with different models for both the LLM, baseline, and judge. 

## Streamlit Demo

How to run:

```bash
streamlit run human/annotator_app.py
```

Home â€” choose a dataset and a model pair (labels are masked so raters are blind).

Rating View â€” side-by-side model outputs with the JSON rendered into readable sections (summary, suggestions, steps, disclaimer). Raters provide Likert scores per rubric dimension and tag errors (e.g., unsafe, non-actionable).

Submit & Persist â€” ratings are appended to out/human/annotations.csv with provenance (example id, pair id, timestamp).

Aggregate â€” use scripts/analyze_human_eval.py to summarize rater agreement and winner rates.

Images of App below:
![alt text](https://github.com/mohandasnj/health-evals/blob/main/img/streamlit1.png)
![alt text](https://github.com/mohandasnj/health-evals/blob/main/img/streamlit2.png)

## ðŸ§  Methods implemented (and evidence)

LLM-as-Judge with self-consistency

  * Code: evals/runners/eval_llm_judge.py, configs/judge.yaml
  
  * Output: out/judged/*.jl
  
Automatic metric-based evals
  
  * Heuristics: evals/runners/eval_auto.py â†’ out/metrics/*.csv

  * Text metrics: evals/runners/eval_ref_metrics.py â†’ out/metrics_ref/*.csv
  (ROUGE-L, BERTScore-F1, embedding cosine, optional PPL)

Human evaluation (blind A/B, Likert, error tags)
  
  * UI: human/annotator_app.py (Streamlit), data: out/human/annotations.csv

  * Aggregation: scripts/analyze_human_eval.py

Scalability

  * Parallel inference: evals/runners/ray_eval.py, shards merged via scripts/collect_ray_outputs.py

Guardrails + schema enforcement

  * Pydantic v2 schema: apps/wellness_coach/schemas.py

  * JSON-only prompts: apps/wellness_coach/prompt_templates/*.jinja

## ðŸš¦ Next Steps / Roadmap (high level goals)

Use health-grounded datasets â€” Add consumer-safe subsets of MultiMedQA, HealthSearchQA, long-form QA; introduce MedSafetyBench-style categories and a curated wellness-coach suite.

Strengthen human annotation â€” Turn the Streamlit app into a rater console with gold items, attention checks, adjudication, and QC reporting (IRR, drift).

Improve judge trustworthiness â€” Increase self-consistency, make prompts style-blind, and run meta-evaluation against human ratings (correlations & overlap).

Health safety & alignment â€” Ship a safety rubric (no diagnosis/dosing, escalation rules) and add automatic policy filters & red-team generators with tracked safety rates.

Failure analysis & adversarial â€” Cluster bad cases, label failure types, and maintain a small adversarial suite to catch regressions.


