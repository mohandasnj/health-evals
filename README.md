# Health-Evals: Scalable Evaluation Harness for Wellness Coaching LLMs

**A reproducible evaluation system** for comparing wellness-coaching LLMs end-to-end:
- A/B **model comparisons** (LLM vs Baseline) with **schema-validated** outputs
- **LLM-as-a-Judge** scoring (rubric + self-consistency)
- **Automatic** safety/quality metrics
- **Reference-based** text metrics (ROUGE-L, BERTScore, embedding cosine, PPL)
- **Human evaluation UI** (blind A/B + Likert + error tags)
- **Ray** for parallel, scalable batch inference
- **Provider-agnostic** (Ollama local, OpenAI hosted, vLLM compatible)

> **Current default config:**  
> **LLM (Model Under Test)** — `llama3:8b` via **Ollama** (local, macOS-friendly)  
> **Baseline & Judge** — **OpenAI** (`gpt-4o-mini` + `gpt-4o`)  
> You can change all of this via **`configs/model.yaml`** only.

---

## 🔎 What this repo does

You give the system a set of daily signals (e.g., RHR/HRV/sleep/steps + a journal snippet).  
It **prompts** two models (LLM & Baseline) with *the same coaching task*, **forces JSON** outputs that match a Pydantic schema (summary → suggestions → steps → disclaimer), then **scores** the results with:
- an **LLM judge** (multi-prompt self-consistency + weighted rubric),
- **cheap automatic checks** (blocked-rate, disclaimer compliance, length, safety regex), and
- **reference-based metrics** against a silver reference set generated by the judge.

All artifacts (raw/parsed outputs, scores, metrics) are written to `out/` so you can track quality diffs over time.

---

## 🧭 Quickstart

> macOS, Python 3.11+, Ollama installed (`brew install ollama`), OpenAI key for baseline/judge

```bash
# 0) Env
python3 -m venv .venv && source .venv/bin/activate
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt
pip install -U "pyarrow==16.1.0" "ray[data]==2.20.0"

# 1) Local LLM
ollama serve            # keep open in its own terminal
ollama pull llama3:8b

# 2) OpenAI (baseline/judge)
echo 'OPENAI_API_KEY=sk-...YOUR_KEY...' >> .env

# 3) Data
python scripts/make_synthetic_data.py
python scripts/prepare_eval_splits.py

# 4) Batch inference (A/B) + Guardrails schema validation
python evals/runners/batch_infer.py

# 5) Judge scoring (rubric + self-consistency)
python evals/runners/eval_llm_judge.py

# 6) Automatic deterministic checks
python evals/runners/eval_auto.py

# 7) Silver references & text metrics
python scripts/refs_from_judge.py --limit 50
python evals/runners/eval_ref_metrics.py --skip-ppl --limit 50

# 8) Human evaluation UI
streamlit run human/annotator_app.py          # saves to out/human/annotations.csv
python scripts/analyze_human_eval.py

# 9) Scale out with Ray

python evals/runners/ray_eval.py \
  --config configs/model.yaml \
  --split evals/datasets/test.jsonl \
  --prompt coach_v1.jinja \
  --tag mut_v1 \
  --outdir out/ray/mut_v1 \
  --num-actors 3 --batch-size 4 --limit 20

python scripts/collect_ray_outputs.py out/ray/mut_v1 out/infer/mut_v1.ray.jsonl
mv out/infer/mut_v1.ray.jsonl out/infer/mut_v1.jsonl
python evals/runners/eval_llm_judge.py
python evals/runners/eval_auto.py
```

## Pipeline Overview

![alt text](https://github.com/mohandasnj/health-evals/blob/main/img/pipeline.png)

## Providers Abstraction

![alt text](https://github.com/mohandasnj/health-evals/blob/main/img/providers.png)

## End-to-End Run

![alt text](https://github.com/mohandasnj/health-evals/blob/main/img/sequence.png)

## Key Files

Configs

  * model.yaml — models & providers (local LLM, baseline, judge)
  
  * judge.yaml — rubric dimensions, weights, scaling

Apps (core logic)

  * providers.py — provider abstraction (Ollama/OpenAI/vLLM) with retries
  
  * wellness_coach/schemas.py — Pydantic v2 schema (Guardrails contracts)
  
  * wellness_coach/prompt_templates/coach_v1.jinja — JSON-only prompt (local)
  
  * wellness_coach/prompt_templates/coach_v2.jinja — coachable variant (baseline)

Evals / Runners

  * batch_infer.py — render prompts → call providers → validate → out/infer/*.jsonl
  
  * eval_llm_judge.py — rubric scoring (N=3 self-consistency) → out/judged/*.jl
  
  * eval_auto.py — deterministic checks → out/metrics/*.csv
  
  * eval_ref_metrics.py — ROUGE-L, BERTScore, embedding cosine, (optional) PPL → out/metrics_ref/*.csv
  
  * ray_eval.py — parallel local inference with Ray

Scripts & Data

  * make_synthetic_data.py, scripts/prepare_eval_splits.py — toy data & splits
  
  * refs_from_judge.py — build silver references via the judge
  
  * xollect_ray_outputs.py — merge Ray shards
  
  * analyze_human_eval.py — aggregate human ratings
  
  * evals/datasets/*.jsonl — input datasets

Human Evaluation

  * annotator_app.py — Streamlit blind A/B rating UI → out/human/annotations.csv

## Results

Look at the files in the out folder corresponding to each statistic. I only put a small test of 5 entries, but there are about 300 entries of test data total. From this small sample, the results show that GPT-4o-mini performs a bit better than Ollama. There could be bias here as well because the judge is another OpenAI model. Further research can be done with different models for both the LLM, baseline, and judge. 

## Streamlit Demo

How to run:

```bash
streamlit run human/annotator_app.py
```

Home — choose a dataset and a model pair (labels are masked so raters are blind).

Rating View — side-by-side model outputs with the JSON rendered into readable sections (summary, suggestions, steps, disclaimer). Raters provide Likert scores per rubric dimension and tag errors (e.g., unsafe, non-actionable).

Submit & Persist — ratings are appended to out/human/annotations.csv with provenance (example id, pair id, timestamp).

Aggregate — use scripts/analyze_human_eval.py to summarize rater agreement and winner rates.

Images of App below:
![alt text](https://github.com/mohandasnj/health-evals/blob/main/img/streamlit1.png)
![alt text](https://github.com/mohandasnj/health-evals/blob/main/img/streamlit2.png)

## 🧠 Methods implemented (and evidence)

LLM-as-Judge with self-consistency

  Code: evals/runners/eval_llm_judge.py, configs/judge.yaml
  
  Output: out/judged/*.jl
  
Automatic metric-based evals
  
  Heuristics: evals/runners/eval_auto.py → out/metrics/*.csv

  Text metrics: evals/runners/eval_ref_metrics.py → out/metrics_ref/*.csv
  (ROUGE-L, BERTScore-F1, embedding cosine, optional PPL)

Human evaluation (blind A/B, Likert, error tags)
  
  UI: human/annotator_app.py (Streamlit), data: out/human/annotations.csv

  Aggregation: scripts/analyze_human_eval.py

Scalability

  Parallel inference: evals/runners/ray_eval.py, shards merged via scripts/collect_ray_outputs.py

Guardrails + schema enforcement

  Pydantic v2 schema: apps/wellness_coach/schemas.py

  JSON-only prompts: apps/wellness_coach/prompt_templates/*.jinja

## 🚦 Next Steps / Roadmap

> This roadmap lays out concrete upgrades to take the system from a toy wellness-coach eval to a rigorous, health-grounded evaluation platform.  
> Items marked **✅ Foundation exists** point to code that already covers part of the work in this repo.

---

### 1) Replace toy prompts with **health-grounded tasks + datasets**

**Goals**
- Evaluate on consumer-facing health questions and long-form coaching aligned with public benchmarks.

**Datasets & Tasks (consumer-safe subsets only)**
- **MultiMedQA**: MedQA, MedMCQA, PubMedQA, MMLU-clinical *(for consumer-interpretable items only)*  
- **HealthSearchQA**: consumer health search questions  
- **Long-form medical QA** benchmarks evaluating LLM-judge vs clinicians *(use consumer-appropriate items; clinician review where needed)*  
- **MedSafetyBench**: safety failure modes (don’t ship content; implement and **report** rubric categories)  
- **Custom Wellness-Coach Suite**: sleep hygiene, stress/mindfulness, activity, nutrition — with constraints (meds, comorbidities, pregnancy, age). Map each item to rubric dimensions: **safety, accuracy, personalization, actionability, empathy**.

**Implementation plan**
- [ ] Add dataset loaders under `data_loaders/` (e.g., `multimedqa.py`, `healthsearchqa.py`).
- [ ] Create **task-specific** prompt templates under `apps/wellness_coach/prompt_templates/health_*`.
- [ ] Add task configs in `configs/tasks/*.yaml` (dataset → prompt → judge rubric).
- [ ] Update runners to accept `--task-config` (route to loaders/templates automatically).
- [ ] Produce per-task reports in `out/reports/<task>/...`.

**Deliverables**
- Per-task judged scores, auto metrics, ref metrics; CSVs + small markdown summaries.
- A `tasks/README.md` documenting data filters and consumer-safety criteria.

**✅ Foundation exists**
- Prompt templating: `apps/wellness_coach/prompt_templates/*`  
- Provider abstraction and runners: `apps/providers.py`, `evals/runners/*`  
- Metrics: `evals/runners/eval_llm_judge.py`, `evals/runners/eval_auto.py`, `evals/runners/eval_ref_metrics.py`

---

### 2) Build the **human-annotation system** (rater console & QC)

**Goals**
- Turn the simple Streamlit app into a full **rater workflow** with quality control and provenance.

**Features**
- **Blind A/B** with randomization; rubric sliders (Likert) + categorical checks.
- **Gold-item injection** and **attention checks**; interleaved **re-ratings** for test–retest reliability.
- **Adjudication flow** for conflicts.
- **Annotator Guide (PDF)** and **QC report** (rater drift, gold pass rates, disagreement heatmaps).
- **Provenance logging**: prompt hash, model+version, judge version, rubric version, task id, slice tags.

**Implementation plan**
- [ ] Extend `human/annotator_app.py` with login, assignment, progress, and QC widgets.  
- [ ] Add `human/schema.py` for a stable rating schema; write to `out/human/ratings.jsonl`.  
- [ ] Build `scripts/qc_humans.py` for drift, gold pass, IRR (Cohen’s κ / Krippendorff’s α).  
- [ ] Generate `docs/annotator_guide.pdf` and `docs/qc_report.md` automatically.

**Deliverables**
- A reproducible human-ratings dataset with QC metrics and IRR.
- A short “How to rate” guide for raters.

**✅ Foundation exists**
- Seed app: `human/annotator_app.py` and aggregation: `scripts/analyze_human_eval.py`  
- Pair builder: `scripts/prepare_human_eval.py`

---

### 3) Make **LLM-as-Judge** trustworthy

**Goals**
- Improve judge robustness and quantify agreement with humans.

**Upgrades**
- Increase **self-consistency** to N>3; **rubric-constrained** judge prompts; **style-blind** judging (strip brand cues and system names).
- **Meta-evaluation**: correlation with human ratings (Pearson/Spearman), error overlap analysis, judge sensitivity to paraphrase; report CIs via bootstrap.
- Maintain **judge versions** and **rubric versions**; pin judge model in `configs/judge.yaml`.

**Implementation plan**
- [ ] Add preprocessor: `evals/judge_pre.py` (normalize outputs, strip brand cues).  
- [ ] Add `--n-judge-prompts` flag and prompt pool in `configs/judge.yaml`.  
- [ ] Script `scripts/compare_judge_vs_human.py` to compute correlations and sensitivity.

**Deliverables**
- A judge reliability report: r with human, sensitivity analysis, and variance under paraphrase.

**✅ Foundation exists**
- Current judge with N=3 and rubrics: `evals/runners/eval_llm_judge.py`, `configs/judge.yaml`

---

### 4) Health-specific safety & alignment (**non-negotiable**)

**Goals**
- Codify health safety constraints into both **rubrics** and **automatic guards**.

**Safety rubric (must check)**
- No diagnosis, no dosing, no treatment plans.  
- Crisis escalation & when to seek care.  
- Source grounding expectations.  
- Contraindication awareness (pregnancy, chest pain, drug interactions).

**Automatic guards**
- [ ] Expand `evals/runners/eval_auto.py` with policy filters & escalation checks.  
- [ ] Add **red-team generators** targeting risks (drug interactions, eating disorders, pediatrics).  
- [ ] Track **unsafe-advice rate**, **missing escalation rate**, **hallucination rate** by task & slice.

**Deliverables**
- `configs/safety_rubric.yaml` and a safety metrics dashboard per task.
- A short **Safety Readiness** summary for each release.

**✅ Foundation exists**
- Deterministic checks & disclaimer compliance: `evals/runners/eval_auto.py`  
- Schema & JSON-only prompts: `apps/wellness_coach/schemas.py`, `apps/wellness_coach/prompt_templates/*`

---

### 5) Failure analysis & adversarial testing

**Goals**
- Systematically study failure modes and protect against regressions.

**Failure analysis**
- [ ] Notebook `notebooks/failure_analysis.ipynb`: cluster bad cases; tag types (unsafe, incorrect, generic, non-actionable, off-policy).  
- [ ] Mine exemplars with links back to `out/infer/*` and judge logs.

**Adversarial sets**
- [ ] Create `evals/datasets/adversarial.jsonl`: prompt injection (“doctor said…”), contradictory symptoms, ambiguous ages, medication traps.  
- [ ] Track performance over time; add to CI smoke tests.

**Deliverables**
- A “Top Failures” report with representative cases and suggested mitigations.
- A small adversarial benchmark run in every PR via GitHub Actions.

**✅ Foundation exists**
- All artifacts are JSONL/CSV for easy slicing: `out/infer/*`, `out/judged/*`, `out/metrics/*`

---


